#!/usr/bin/env npx tsx
/**
 * Import Playbook - å¯¼å…¥çˆ¬è™«æ•°æ®åˆ°æ•°æ®åº“
 *
 * åŠŸèƒ½:
 * - è¯»å– crawl_playbook çš„ JSON/YAML è¾“å‡º
 * - åˆ›å»º/æ›´æ–° source è®°å½•
 * - åˆ›å»º source_version è®°å½•ï¼ˆå¦‚æœå·²å­˜åœ¨åˆ™æ–°å»ºç‰ˆæœ¬ï¼‰
 * - åˆ›å»º documents è®°å½•
 * - åˆ›å»º chunks è®°å½•ï¼ˆ1:1 å¯¹åº” documentï¼Œå¸¦ embeddingï¼‰
 * - åˆ›å»º build_task è®°å½•
 *
 * ä½¿ç”¨æ–¹æ³•:
 *   npx tsx scripts/import-playbook.ts <json_file>
 *   npx tsx scripts/import-playbook.ts output/sites/arxiv.org/crawl_playbooks/crawl_playbook_xxx.json
 *
 * ç¯å¢ƒå˜é‡:
 *   DATABASE_URL - æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²
 *   OPENAI_API_KEY æˆ– OPENROUTER_API_KEY - ç”¨äºç”Ÿæˆ embedding
 */

import fs from "fs";
import path from "path";
import crypto from "crypto";
import {
  getDb,
  sources,
  sourceVersions,
  documents,
  chunks,
  buildTasks,
  eq,
  desc,
  sql,
} from "@actionbookdev/db";
import type {
  SourceVersionStatus,
  DocumentStatus,
} from "@actionbookdev/db";

// åŠ è½½ç¯å¢ƒå˜é‡ (åŒæ—¶åŠ è½½ action-builder å’Œ knowledge-builder çš„ .env)
import * as dotenv from "dotenv";
// å…ˆåŠ è½½ action-builder çš„ .env (DATABASE_URL ç­‰)
dotenv.config({ path: path.resolve(process.cwd(), ".env") });
// å†åŠ è½½ knowledge-builder çš„ .env (OPENAI_API_KEY ç­‰)ï¼Œä¸è¦†ç›–å·²æœ‰çš„
dotenv.config({ path: path.resolve(process.cwd(), "../knowledge-builder/.env") });

// ============================================================================
// Types
// ============================================================================

interface PageInfo {
  url: string;
  title: string;
  depth: number;
  features: string[];
  links: string[];
}

interface CrawlPlaybookData {
  startUrl: string;
  domain: string;
  totalPages: number;
  pages: PageInfo[];
}

interface ImportResult {
  sourceId: number;
  versionId: number;
  buildTaskId: number;
  documentsCreated: number;
  chunksCreated: number;
}

// ============================================================================
// Embedding Service (OpenAI - compatible with knowledge-builder)
// ============================================================================

import OpenAI from "openai";
import { HttpsProxyAgent } from "https-proxy-agent";

const EMBEDDING_MODEL = "text-embedding-3-small";
const EMBEDDING_DIMENSION = 1536;
const BATCH_SIZE = 100;

class EmbeddingService {
  private client: OpenAI;
  readonly model = EMBEDDING_MODEL;
  readonly dimension = EMBEDDING_DIMENSION;

  constructor() {
    // ä½¿ç”¨ OpenAI API (ä¸ knowledge-builder ä¿æŒä¸€è‡´)
    const apiKey = process.env.OPENAI_API_KEY;
    if (!apiKey) {
      throw new Error(
        "OPENAI_API_KEY not found. Please set it in .env file."
      );
    }

    const proxyUrl = process.env.HTTPS_PROXY || process.env.HTTP_PROXY;
    const baseUrl = process.env.OPENAI_BASE_URL;

    if (proxyUrl) {
      console.log(`[Embedding] Using proxy: ${proxyUrl}`);
    }
    if (baseUrl) {
      console.log(`[Embedding] Using custom baseURL: ${baseUrl}`);
    }

    this.client = new OpenAI({
      apiKey,
      baseURL: baseUrl,
      timeout: 60000,
      maxRetries: 3,
      httpAgent: proxyUrl ? new HttpsProxyAgent(proxyUrl) : undefined,
    });

    console.log(`[Embedding] Using OpenAI ${this.model} (${this.dimension} dim)`);
  }

  async getEmbedding(text: string): Promise<number[]> {
    const trimmed = text.trim();
    if (!trimmed) {
      throw new Error("Cannot embed empty text");
    }

    const response = await this.client.embeddings.create({
      model: this.model,
      input: trimmed,
    });

    return response.data[0].embedding;
  }

  async getEmbeddings(texts: string[]): Promise<number[][]> {
    const validTexts = texts.map((t) => t.trim()).filter((t) => t.length > 0);
    if (validTexts.length === 0) return [];

    console.log(`[Embedding] Processing ${validTexts.length} texts`);

    const results: number[][] = [];

    // Process in batches
    for (let i = 0; i < validTexts.length; i += BATCH_SIZE) {
      const batch = validTexts.slice(i, i + BATCH_SIZE);

      const response = await this.client.embeddings.create({
        model: this.model,
        input: batch,
      });

      for (const data of response.data) {
        results.push(data.embedding);
      }

      console.log(`   Embedding progress: ${Math.min(i + BATCH_SIZE, validTexts.length)}/${validTexts.length}`);

      // Simple rate limiting
      if (i + BATCH_SIZE < validTexts.length) {
        await new Promise((r) => setTimeout(r, 100));
      }
    }

    return results;
  }
}

// ============================================================================
// Helper Functions
// ============================================================================

function md5(text: string): string {
  return crypto.createHash("md5").update(text).digest("hex");
}

function estimateTokens(text: string): number {
  // ç®€å•ä¼°ç®—ï¼šè‹±æ–‡çº¦ 4 å­—ç¬¦/tokenï¼Œä¸­æ–‡çº¦ 2 å­—ç¬¦/token
  return Math.ceil(text.length / 3);
}

function formatPageAsMarkdown(page: PageInfo): string {
  const lines: string[] = [];
  lines.push(`# ${page.title || "Untitled"}`);
  lines.push("");
  lines.push(`**URL:** ${page.url}`);
  lines.push(`**Depth:** ${page.depth}`);
  lines.push("");
  lines.push("## é¡µé¢åŠŸèƒ½");
  lines.push("");
  page.features.forEach((f, i) => {
    lines.push(`${i + 1}. ${f}`);
  });
  return lines.join("\n");
}

function formatContentText(page: PageInfo): string {
  return page.features.join("\n");
}

// ============================================================================
// Import Logic
// ============================================================================

async function importPlaybook(filePath: string): Promise<ImportResult> {
  // è¯»å–æ–‡ä»¶
  console.log(`\nğŸ“– è¯»å–æ–‡ä»¶: ${filePath}`);
  const content = fs.readFileSync(filePath, "utf-8");
  const data: CrawlPlaybookData = JSON.parse(content);

  console.log(`   åŸŸå: ${data.domain}`);
  console.log(`   èµ·å§‹ URL: ${data.startUrl}`);
  console.log(`   æ€»é¡µé¢æ•°: ${data.totalPages}`);

  const db = getDb();
  const embeddingService = new EmbeddingService();

  // 1. åˆ›å»ºæˆ–è·å– source
  console.log(`\nğŸ“¦ åˆ›å»º/è·å– Source...`);
  let source = await db
    .select()
    .from(sources)
    .where(eq(sources.domain, data.domain))
    .limit(1)
    .then((rows) => rows[0]);

  if (source) {
    console.log(`   å·²å­˜åœ¨ source #${source.id}ï¼Œå°†æ–°å»ºç‰ˆæœ¬`);
  } else {
    const result = await db
      .insert(sources)
      .values({
        name: data.domain,
        baseUrl: data.startUrl,
        domain: data.domain,
        description: `Playbook crawl data for ${data.domain}`,
        crawlConfig: {
          maxDepth: 3,
          maxPages: 50,
        },
      })
      .returning();
    source = result[0];
    console.log(`   åˆ›å»º source #${source.id}`);
  }

  // 2. åˆ›å»º source_version
  console.log(`\nğŸ“‹ åˆ›å»º Source Version...`);
  const latestVersion = await db
    .select({ versionNumber: sourceVersions.versionNumber })
    .from(sourceVersions)
    .where(eq(sourceVersions.sourceId, source.id))
    .orderBy(desc(sourceVersions.versionNumber))
    .limit(1);

  const nextVersionNumber = (latestVersion[0]?.versionNumber ?? 0) + 1;

  const versionResult = await db
    .insert(sourceVersions)
    .values({
      sourceId: source.id,
      versionNumber: nextVersionNumber,
      status: "building" as SourceVersionStatus,
      commitMessage: `Import from playbook: ${path.basename(filePath)}`,
      createdBy: "import-playbook",
    })
    .returning();
  const version = versionResult[0];
  console.log(`   åˆ›å»º version #${version.id} (v${nextVersionNumber})`);

  // 3. åˆ›å»º build_task
  console.log(`\nğŸ“ åˆ›å»º Build Task...`);
  const taskResult = await db
    .insert(buildTasks)
    .values({
      sourceId: source.id,
      sourceUrl: data.startUrl,
      sourceName: data.domain,
      sourceCategory: "playbook",
      stage: "knowledge_build",
      stageStatus: "running",
      config: {
        importedFrom: filePath,
        totalPages: data.totalPages,
      },
      knowledgeStartedAt: new Date(),
    })
    .returning();
  const buildTask = taskResult[0];
  console.log(`   åˆ›å»º build_task #${buildTask.id}`);

  // 4. å‡†å¤‡æ‰€æœ‰é¡µé¢å†…å®¹ç”¨äºæ‰¹é‡ç”Ÿæˆ embedding
  console.log(`\nğŸ§  Generating Embeddings...`);
  const pageContents = data.pages.map((page) => formatContentText(page));
  const embeddings = await embeddingService.getEmbeddings(pageContents);
  console.log(`   Generated ${embeddings.length} embeddings (${embeddings[0]?.length || 0} dim)`);

  // 5. åˆ›å»º documents å’Œ chunks
  console.log(`\nğŸ“„ åˆ›å»º Documents å’Œ Chunks...`);
  let documentsCreated = 0;
  let chunksCreated = 0;

  for (let i = 0; i < data.pages.length; i++) {
    const page = data.pages[i];
    const embedding = embeddings[i];
    const contentText = formatContentText(page);
    const contentMd = formatPageAsMarkdown(page);
    const urlHash = md5(page.url);
    const contentHash = md5(contentText);

    // åˆ›å»º document
    const docResult = await db
      .insert(documents)
      .values({
        sourceId: source.id,
        sourceVersionId: version.id,
        url: page.url,
        urlHash,
        title: page.title || "Untitled",
        description: page.features[0] || "",
        contentText,
        contentHtml: "", // playbook ä¸ä¿å­˜ HTML
        contentMd,
        depth: page.depth,
        breadcrumb: [],
        wordCount: contentText.length,
        language: "zh",
        contentHash,
        status: "active" as DocumentStatus,
        version: 1,
      })
      .returning();
    const doc = docResult[0];
    documentsCreated++;

    // åˆ›å»º chunk (1:1)
    const embeddingStr = `[${embedding.join(",")}]`;
    await db.execute(sql`
      INSERT INTO chunks (
        document_id, source_version_id, content, content_hash, chunk_index,
        start_char, end_char, heading, heading_hierarchy,
        token_count, embedding, embedding_model
      ) VALUES (
        ${doc.id},
        ${version.id},
        ${contentText},
        ${contentHash},
        ${0},
        ${0},
        ${contentText.length},
        ${page.title || "Untitled"},
        ${JSON.stringify([{ level: 1, text: page.title || "Untitled" }])}::jsonb,
        ${estimateTokens(contentText)},
        ${embeddingStr}::vector,
        ${"text-embedding-3-small"}
      )
    `);
    chunksCreated++;

    // è¿›åº¦æ˜¾ç¤º
    if ((i + 1) % 10 === 0 || i === data.pages.length - 1) {
      console.log(`   è¿›åº¦: ${i + 1}/${data.pages.length}`);
    }
  }

  // 6. æ›´æ–°ç‰ˆæœ¬å’Œä»»åŠ¡çŠ¶æ€ (knowledge_build é˜¶æ®µå®Œæˆï¼Œç­‰å¾… action_build)
  console.log(`\nâœ… å®Œæˆ Knowledge Build é˜¶æ®µ...`);

  // ç‰ˆæœ¬çŠ¶æ€ä¿æŒ 'building'ï¼Œç­‰å¾… action-builder å®Œæˆåå†å‘å¸ƒä¸º 'active'
  // source_versions.status = 'building' (å·²ç»æ˜¯é»˜è®¤å€¼ï¼Œä¸éœ€è¦æ›´æ–°)

  // æ›´æ–° source çš„ lastCrawledAt
  await db
    .update(sources)
    .set({
      lastCrawledAt: new Date(),
      updatedAt: new Date(),
    })
    .where(eq(sources.id, source.id));

  // æ›´æ–° build_task: knowledge_build é˜¶æ®µå®Œæˆ
  await db
    .update(buildTasks)
    .set({
      stage: "knowledge_build",
      stageStatus: "completed",
      knowledgeCompletedAt: new Date(),
      updatedAt: new Date(),
    })
    .where(eq(buildTasks.id, buildTask.id));

  return {
    sourceId: source.id,
    versionId: version.id,
    buildTaskId: buildTask.id,
    documentsCreated,
    chunksCreated,
  };
}

// ============================================================================
// CLI
// ============================================================================

async function main(): Promise<void> {
  const args = process.argv.slice(2);

  if (args.length === 0 || args[0] === "--help" || args[0] === "-h") {
    console.log(`
Import Playbook - å¯¼å…¥çˆ¬è™«æ•°æ®åˆ°æ•°æ®åº“

ç”¨æ³•:
  npx tsx scripts/import-playbook.ts <json_file>

ç¤ºä¾‹:
  npx tsx scripts/import-playbook.ts output/sites/arxiv.org/crawl_playbooks/crawl_playbook_xxx.json

ç¯å¢ƒå˜é‡:
  DATABASE_URL       - æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²
  OPENAI_API_KEY     - OpenAI API Key (ç”¨äº embedding)
  OPENROUTER_API_KEY - OpenRouter API Key (ä¼˜å…ˆä½¿ç”¨)
`);
    process.exit(0);
  }

  const filePath = args[0];

  // éªŒè¯æ–‡ä»¶å­˜åœ¨
  if (!fs.existsSync(filePath)) {
    console.error(`âŒ æ–‡ä»¶ä¸å­˜åœ¨: ${filePath}`);
    process.exit(1);
  }

  // éªŒè¯æ–‡ä»¶ç±»å‹
  if (!filePath.endsWith(".json")) {
    console.error(`âŒ åªæ”¯æŒ JSON æ–‡ä»¶`);
    process.exit(1);
  }

  console.log("=".repeat(60));
  console.log("Import Playbook - å¯¼å…¥çˆ¬è™«æ•°æ®åˆ°æ•°æ®åº“");
  console.log("=".repeat(60));

  try {
    const result = await importPlaybook(filePath);

    console.log("\n" + "=".repeat(60));
    console.log("å¯¼å…¥å®Œæˆï¼");
    console.log("=".repeat(60));
    console.log(`ğŸ“¦ Source ID: ${result.sourceId}`);
    console.log(`ğŸ“‹ Version ID: ${result.versionId}`);
    console.log(`ğŸ“ Build Task ID: ${result.buildTaskId}`);
    console.log(`ğŸ“„ Documents: ${result.documentsCreated}`);
    console.log(`ğŸ§© Chunks: ${result.chunksCreated}`);

    process.exit(0);
  } catch (error) {
    console.error("\nâŒ å¯¼å…¥å¤±è´¥:", error);
    process.exit(1);
  }
}

main();

